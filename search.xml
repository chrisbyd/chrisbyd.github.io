<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Langrange Multipliers for Constrained Optimization</title>
      <link href="/2019/05/12/Langrange/"/>
      <url>/2019/05/12/Langrange/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="We-have-gone-through-a-few-optimization-methods-for-unconstrained-optimization-However-in-reality-tremendous-problems-are-factored-into-a-serires-of-constrained-optimization-problems-Therefore-in-this-post-I-will-mainly-introduce-a-widely-employed-method-in-various-machine-learning-algorithms"><a href="#We-have-gone-through-a-few-optimization-methods-for-unconstrained-optimization-However-in-reality-tremendous-problems-are-factored-into-a-serires-of-constrained-optimization-problems-Therefore-in-this-post-I-will-mainly-introduce-a-widely-employed-method-in-various-machine-learning-algorithms" class="headerlink" title="We have gone through a few optimization methods for unconstrained optimization. However, in reality, tremendous problems are factored into a serires of constrained optimization problems. Therefore, in this post, I will mainly introduce a widely employed method in various machine learning algorithms."></a>We have gone through a few optimization methods for unconstrained optimization. However, in reality, tremendous problems are factored into a serires of constrained optimization problems. Therefore, in this post, I will mainly introduce a widely employed method in various machine learning algorithms.</h2><h2 id="Langrange-Multiplier"><a href="#Langrange-Multiplier" class="headerlink" title="Langrange Multiplier"></a>Langrange Multiplier</h2><p>###Intrduction </p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, unconstrained optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Proximal_policy_optimization</title>
      <link href="/2019/05/11/Proximal-policy-optimization/"/>
      <url>/2019/05/11/Proximal-policy-optimization/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Trust_region_policy_optimization</title>
      <link href="/2019/05/11/Trust-region-policy-optimization/"/>
      <url>/2019/05/11/Trust-region-policy-optimization/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Natural_policy_gradient</title>
      <link href="/2019/05/11/Natural-policy-gradient/"/>
      <url>/2019/05/11/Natural-policy-gradient/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Policy_gradient</title>
      <link href="/2019/05/11/Policy-gradient/"/>
      <url>/2019/05/11/Policy-gradient/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimization methods(4)</title>
      <link href="/2019/05/11/Optimization-4/"/>
      <url>/2019/05/11/Optimization-4/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>In this post, I will give a brief introduction on another popular optimization method - <strong><em>trust region optimization method</em></strong>. In the last few posts, I elaborated on <em>line search</em>, <em>steepest gradient descent</em> and <em>newton’s method</em>. However,both the optimized steepest gradient descent and newton’s method are based on the assumption that the objective function can be approximated locally by a quadratic function. However, it fails to approximate the objective function ,the descent may leads to the convergence problem as the following two pics show.</p><p><figure class="half"><br>    <img src="/images/Trustregion1.png"><br>    <img src="/images/Trustregion2.png"><br></figure></p><h2 id="To-address-this-problem-we-will-introduce-trust-region-method-in-this-post"><a href="#To-address-this-problem-we-will-introduce-trust-region-method-in-this-post" class="headerlink" title="To address this problem, we will introduce trust region method in this post."></a>To address this problem, we will introduce <strong>trust region method</strong> in this post.</h2><h2 id="Trust-region-methods"><a href="#Trust-region-methods" class="headerlink" title="Trust region methods"></a>Trust region methods</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><font color="red"><strong>Introduction</strong></font></h3><blockquote><p>Trust-region method (TRM) is one of the most important numerical optimization methods in solving nonlinear programming (NLP) problems. It works in a way that first define a region around the current best solution, in which a certain model (usually a quadratic model) can to some extent approximate the original objective function. TRM then take a step forward according to the model depicts within the region. Unlike the line search methods, TRM usually determines the step size before the improving direction (or at the same time). If a notable decrease (our following discussion will based on minimization problems) is gained after the step forward, then the model is believed to be a good representation of the original objective function. If the improvement is too subtle or even a negative improvement is gained, then the model is not to be believed as a good representation of the original objective function within that region. The convergence can be ensured that the size of the “trust region” (usually defined by the radius in Euclidean norm) in each iteration would depend on the improvement previously made.</p></blockquote><h3 id="Deduction"><a href="#Deduction" class="headerlink" title="Deduction"></a><font color="red"><strong>Deduction</strong></font></h3><p>Consider the unconstrained problem:<br>$$min\;f(x), x \in \mathbb{R}^n$$<br>We compute the second order <strong>Taylor expansion</strong> approximation of the objective at point $x_k$ ,namely:<br>$$f(x)\approx f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{1}{2}\nabla^2f(x_k)(x-x_k)^2$$<br>Denote $d=x-x_k$, we have the quadratic function:<br>$$\phi_k(d)=f(x_k)+\nabla f(x_k)^Td +\frac{1}{2}d^T\nabla^2 f(x_k)d$$<br>To accurately approximate $f(x_k+d)$ with $\phi_k(d)$ in the vicinity of $x_k$, we enforce constraints on $d$ with $\left|d\right|d \leq r_k$. $r_k$ is a given constant which in this setting is widely referred to as the <strong>trust region radius</strong>. The constraint on $d$ could be various kinds of norms according to different settings. In this post, we will focus on the method which enforces <em>Ecludian norm</em> on $d$, which means $(d^Td)^{\frac{1}{2}}\leq r_k$.<br>If $d_k$ is the optimum solution to the objective, then, there exits a mutiplier $w \geq 0$ satisfying<br>$$ \nabla^2 f(x_k)d_k +\nabla f(x_k) +\frac{w’}{d_k^Td_k}d_k = 0$$<br>$$ w’(\left||d_k\right||-r_k) = 0)$$<br>Denote $$w=\frac{}$$</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, unconstrained optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimization methods(3)</title>
      <link href="/2019/05/09/Optimization-3/"/>
      <url>/2019/05/09/Optimization-3/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>In this post, I will talk about another optimzation method which is called <strong><em>Conjugate gradient descent</em></strong>. The reason for the induction here is that <strong><em>CG</em></strong> method is employed in reinforcement learning but is not well explained in most blogs and books. I have read quite a lot descriptions from various </p><h2 id="Conjugate-Gradient-Descent"><a href="#Conjugate-Gradient-Descent" class="headerlink" title="Conjugate Gradient Descent"></a>Conjugate Gradient Descent</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><blockquote><p><strong>Conjugate gradient descent</strong> is a prominent iterative method for solving sparse systems of linear equations. It basically optimizes the function in the order of \(N\) A-orthogonal search directions. Intuitively, it minimizes the function along one direction \(d\) to the point where the subsequent minimization steps will never need to minimize over that direction again.</p></blockquote><h3 id="Conjugacy"><a href="#Conjugacy" class="headerlink" title="Conjugacy"></a>Conjugacy</h3><ul><li><strong>Conjugation definition</strong></li></ul><blockquote><p>Suppose A is a \(n*n\) positive definite matrix. \(d^1\) and \(d^2\) are two directions in \(R^n\) which satisfy the following equation:<br>$$d^1Ad^2=0$$<br>Then these two directions are called conjugate directions of \(A\).</p></blockquote><ul><li><strong>Conjugate optimization</strong></li></ul><p>Steepest gradient descent as we all know will take the zigzag path in the optimization process. It will take steps in the direction which has been taken before. Wouldnt it be better if every time we take a step, we have arrived at a poistion where we never would need to step in that direction again. The idea is demonstrated as follows.) </p><ol><li>Pick a set of orthogonal <em>search directions</em> \(d_0,d_1, …., d_{n-1}\)</li><li><p>In each direction, we take exactly one step to minimize the function.</p><p>$$x_{i+1}=x_i+\alpha_i d_i$$</p></li></ol><p>We denote the <em>error item</em> after \(i-1\) iterations as \(e_i\), as illustrated in the above picture. To find the value of \(\alpha_i\) , use the fact that \(e_{i+1}\) should be orthogonal to \(d_i\), so that we never step in the direction of \(d_i\) again.  Then, we have</p><p>  $$<br>  \begin{split}<br>   d_i^Te_{i+1}=0  \\<br>   d_i^T(e_i+\alpha_id_i)=0 \\<br>   \alpha_i =- \frac{d_i^Te_i}{d_i^Td_i}<br>  \end{split}<br>  $$</p><ol start="3"><li><p>After n steps, we achieve the minimum.</p><p>However, the problem is not solved since we dont know about \(e_i\) in advance. Now we relax the <em>orthogonal</em> condition to <em>A-orthogonal,conjugate</em> ,which, as stated before, means that $d_i^TAd_j=0$.</p></li></ol><p><img src="/images/Conjugate1.png" alt="Conjugate directions"><br>We set the directional derivative to zero:</p><p>$$<br>\begin{split}<br>  \frac{d}{d\alpha}f(x_{i+1}) \\<br>  f’(x_{i+1})^T\frac{d}{d\alpha}x_{i+1} =0 \\<br>  -r_{i+1}^Td_i =0<br>\end{split}<br>$$</p><p>$$ d_i^TAe_{i+1} =0$$<br>Then, here is the expression for $\alpha_i$ when the search directions are <em>A_orthogonal(Conjugate)</em>:<br>$$<br>\begin{align}<br>\alpha_i= -\frac{d_i^TAe_i}{d_i^TAd_i} \<br>        = \frac{d_i^Tr_i}{d_i^TAd_i}<br>\end{align}<br>$$<br>Note that if the search vectors are just the residuals, this formula would be identical to the <em>steepest Descent</em>.<br>To prove this procedure indeed compute x in $n$ steps, we can express the error item as a linear combination of search directions since the search directions are a complete span of the entire search space; namely:</p><p>$$e_0 = \sum_{j=0}^{n-1}\delta_j d(j)$$</p><p>The corresponding values of each $\delta_i$ can be computed using the fact that the search directions are <em>conjugate</em>. We premultiply the above expression by $d_k^TA$:</p><p>$$<br>\begin{align}<br>d_k^TAe_0=\sum_j\delta_j d_k^TAd_j \\<br>d_k^TAe_0=\delta_k d_k^TAd_k  \\<br>  \delta_k= \frac{d_k^TAe_0}{d_k^TAd_k} \\<br>  =\frac{d_k^TA(e_0+\sum_{i=0}^{k-1}\alpha_i d_i)}{d_k^TAd_k} \\<br>  =\frac{d_k^TAe_k}{d_k^TAd_k} \\<br>\end{align}<br>$$<br>By the above equation, we find that $\alpha_i =-\delta_i$. As the following equation shows, the process of building up x component by component can also be viewed as cutting down the error term component by component. See the following Figure.</p><p>$$<br>\begin{align}<br>e_i = e_0+\sum_{j=0}^{i-1}\alpha_j d_j \\<br>    = \sum_{j=0}^{n-1}\delta_j d_j - \sum_{j=0}^{i-1}\delta_j d_j \\<br>    =\sum_{j=i}^{n-1}\delta_j d_j \\<br>\end{align}<br>$$</p><p>After n iterations, every error term is cut away, and $e_n=0$; the proof is complete. </p><p><img src="/images/Conjugate2.png" alt></p><ul><li><strong>Conjugate directions</strong><br>According to the above demonstration, we have a clear idea about the conjugate optimization. The key point is how to find a set of <em>conjugate directions</em> $d_i$. We will briefly go through a commmon method to tackle this headache which is called <strong>conjugate Gram-Schmidt</strong> process.<br>Suppose we have a set of n linearly independent vectors $u_0,u_1,…,u_{n-1}$. To construct $d_i$, take $u_i$ and substract out any component that are not conjugate to the previous $d$ vectors. See the following fig.<br><img src="/images/Conjugate3.png" alt><br>In other words, set $d_0=u_0$, and for $i&gt;0$, set</li></ul><p>$$d_i=u_i + \sum_{k=0}^{i-1}\beta_{ik} d_k$$</p><p>where the $\beta_{ik}$ are defined for $i&gt;k$. To figure out their values, we employ the same trick for finding $\delta_j$, namely: </p><p>$$<br>\begin{align}<br> d_i^TAd_j = u_i^TAd_j +\sum_{k=0}^{i-1}\beta_{ik} d_k^TAd_j \\<br> 0 = u_i^TAd_j +\beta_ {ij}d_j^TAd_j \\<br> \beta_{ij} = -\frac{u_i^TAd_j}{d_j^TAd_j}<br>\end{align}<br>$$<br>The weakness of this method lies in the fact that all old search directions need to be kept in memory to contruct a new direction. </p><h3 id="Conjugate-gradient-method"><a href="#Conjugate-gradient-method" class="headerlink" title="Conjugate gradient method"></a><font color="#dd0000">Conjugate gradient method</font><br></h3><p>According to the above demonstration, we need to find a set of independent vectors. In this section, I will demonstrate the <strong>Conjugate gradient method</strong> way to deal with all of these issues. However, since the detailed proof of the intrinsics of the algorithm is too length, I will skip the proof and get hands on the algorithm directly. </p><ul><li><p>Get the first direction $d_0 = r_0 =b -Ax_0$</p></li><li><p>Compute the step size in that direction which is $$\alpha_i = \frac { r_i^Tr_i }{ d_i^TAd_i }$$</p></li><li><p>Get the position of next point $x_{i+1} = x_i +\alpha_i d_i$</p></li><li><p>Compute the gradient (residual) at $x_{i+1}$ which equals $r_{i+1} = r_i-\alpha_i Ad_i$</p></li><li><p>Compute the coefficient of $d_i$, namely $$\beta_{i+1} = \frac{r_{i+1}^Tr_{i+1}}{r_i^Tr_i}$$</p></li><li><p>Get the next search direction $d_{i+1} = r_{i+1} + \beta_{i+1} d_i$</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, unconstrained optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Diary</title>
      <link href="/2019/05/07/Diary/"/>
      <url>/2019/05/07/Diary/</url>
      
        <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my diary, enter password to enter." />    <label for="pass">Welcome to my diary, enter password to enter.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1+yfwiVv3/56zjLVYIuh1egBFsKv/1Y9MkD+CgQI7ooHLZyg2ds0wmH+QXxfG3negrRwA7uSLjYJWFGs7KfB6nhOvNaBN4AbUaLig+Ac8H4gtlhvA+3pbOq23T4OUXFiga2xJVbKJubS+NuLNBkFDy4E5QGsYlSUpY=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diary, phd life </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimization methods(2)</title>
      <link href="/2019/05/07/optimization-2/"/>
      <url>/2019/05/07/optimization-2/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>This is the second post to the general optimization methods. In this post, I will briefly talk about <strong><em>Newton’s method</em></strong> .</p><h2 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h2><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><blockquote><p>In numerical analysis, Newton’s method, also known as the <strong>Newton–Raphson method</strong>, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.</p></blockquote><h3 id="Deduction"><a href="#Deduction" class="headerlink" title="Deduction"></a>Deduction</h3><p>Suppose we have a function \(f(x)\) which is real and differentiable. We want to find the solutiont to \(f(x)=0\). Lets suppose we have an initial guess for the solution, say \(x_0\). The initial guess is not that satisfactory but we can use it to iteratively get a more accurate guess. Below is the deduction process.</p><ol><li>We first use <strong>Taylor Expansion</strong> to get the fist order expansion of \(f(x)\)</li></ol><p>$$y=f(x_0)+f’(x_0)(x-x_0)$$</p><ol start="2"><li><p>Set \(y=0\) , which equals to\(f(x_0)+f’(x_0)(x-x_0)=0\)and we can get the following iterative optimization equation<br>$$x=x_0 - \frac{f(x_0)}{f’(x_0)}$$</p></li><li><p>By employ the above equation, we can approximate the root of the function \(f(x)=0\) after sufficient iterations.  The <strong>Newton’s Method</strong> is graphically depicted in the picture below.<br><img src="/images/newton.png" alt="newton"> </p></li></ol><h3 id="Newton’s-method-for-optimization"><a href="#Newton’s-method-for-optimization" class="headerlink" title="Newton’s method for optimization"></a>Newton’s method for optimization</h3><p>If we want to minimize a function, say \(f(x)\) instead of searching for the roots of the function. Intuitively, we can employ <strong>Newton’s Method</strong> on the gradient of the function \(f(x)\) since the minimum of the function \(f(x)\) is achieved when the gradient of the function equals to \(0\).</p><ul><li>We first get the second order <strong>Taylor Expansion</strong> of the function \(f(x)\) at the initial point \(x_0\). Suppose the function is multi-variant.</li></ul><p>$$f(x)=f(x_0)+\nabla f(x_0)(x-x_0)+ \frac{1}{2}(x-x_0)^T \nabla^2 f(x_0) (x-x_0)$$</p><p>\(\nabla^2 f(x_0)\) is the <strong>Hessian Matrix</strong> or <strong>Fisher information matrix)</strong> of \(f(x)\) at \(x_0\). </p><ul><li>then  we need to apply <strong>Newton’s Method</strong> to find the root of \(\nabla f(x) =0\), which equals to the following :<br>$$\nabla f(x_0)+\nabla^2 f(x_0)(x-x_0)=0$$<br>Then , we can derive the iterative update equation as <strong> \(x_1=x_0 - \frac{\nabla f(x_0)}{\nabla^2 f(x_0)}\) </strong></li></ul><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><strong>Discussion</strong></h3><h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h4><p>The convergence speed is way faster than <strong>Steepest Gradient Descent</strong>.</p><h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><p>Need to compute the Hessian, which is rather compuation-intensive. Meanwhile, it relys on the second order Taylor expansion to approximate the original function. Thus, the scenario may happen where the quadratic approximation function.  </p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, unconstrained optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimization methods(1)</title>
      <link href="/2019/05/07/gradient/"/>
      <url>/2019/05/07/gradient/</url>
      
        <content type="html"><![CDATA[<p>This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients.</p><h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><blockquote><p>Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard.</p></blockquote><h2 id="Optimzation-methods"><a href="#Optimzation-methods" class="headerlink" title="Optimzation methods"></a>Optimzation methods</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a><strong>Gradient Descent</strong></h3><blockquote><p>Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. </p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible.</p><div style="text-align: center"><br><img src="/images/mountain.png"><br></div><p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.</p><h3 id="An-example"><a href="#An-example" class="headerlink" title="An example"></a><em>An example</em></h3><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>Given the cost function</p><p>$$f(m,b)=\frac{1}{N}\sum_{i=1}^{n} (x)$$</p><p>The gradient can be calculated as:</p><div style="text-align: center"><br><img src="/images/gradient.png"><br></div><p>To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.</p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a><strong>Stochastic Gradient Descent</strong></h3><p>Conputing the gradient over all the data points is way too expensive. Stochastic  gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update.</p><h3 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><h3 id="Natural-Gradient"><a href="#Natural-Gradient" class="headerlink" title="Natural Gradient"></a>Natural Gradient</h3>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, optimization </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
