<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Diary</title>
      <link href="/2019/05/07/Diary/"/>
      <url>/2019/05/07/Diary/</url>
      
        <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my diary, enter password to enter." />    <label for="pass">Welcome to my diary, enter password to enter.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX19NbdjGj/7yYSRpZnkdOUyCYd1rlcJXk3Gbf2x81CfX7tuaQHRsTcIQZTNK9qSUvpc+Spqc36f+ZPVPF+HIZR/TstS2TtFYtdtxxb/o/IpuuiyLbFEOxiew2Uc9n7gwBEl/cEAIRjc8CNjHBW+YkJNdfKmO7jWNi6s=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diary, phd life </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>optimization_2</title>
      <link href="/2019/05/07/optimization-2/"/>
      <url>/2019/05/07/optimization-2/</url>
      
        <content type="html"><![CDATA[<p>This is the second post to the general optimization methods. In this post, I will briefly talk about <strong><em>Newton’s method</em></strong> .</p><h2 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h2><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><p>In numerical analysis, Newton’s method, also known as the <strong>Newton–Raphson method</strong>, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimization methods(1)</title>
      <link href="/2019/05/07/gradient/"/>
      <url>/2019/05/07/gradient/</url>
      
        <content type="html"><![CDATA[<p>This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients.</p><h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><blockquote><p>Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard.</p></blockquote><h2 id="Optimzation-methods"><a href="#Optimzation-methods" class="headerlink" title="Optimzation methods"></a>Optimzation methods</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a><strong>Gradient Descent</strong></h3><blockquote><p>Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. </p></blockquote><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible.</p><div style="text-align: center"><br><img src="/images/mountain.png"><br></div><p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.</p><h3 id="An-example"><a href="#An-example" class="headerlink" title="An example"></a><em>An example</em></h3><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>Given the cost function</p><p>$$f(m,b)=\frac{1}{N}\sum_{i=1}^{n} (x)$$</p><p>The gradient can be calculated as:</p><div style="text-align: center"><br><img src="/images/gradient.png"><br></div><p>To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.</p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a><strong>Stochastic Gradient Descent</strong></h3><p>Conputing the gradient over all the data points is way too expensive. Stochastic  gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update.</p><h3 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><h3 id="Natural-Gradient"><a href="#Natural-Gradient" class="headerlink" title="Natural Gradient"></a>Natural Gradient</h3>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning, optimization </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
