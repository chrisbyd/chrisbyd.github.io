<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>Chris</title>
  
  
  <meta name="description" content="This is the place for chris to write down his daily feelings and some technical articles">
  

  

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">
  

  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.4.19/css/style.css">
  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
  
  
  <div class="cover-wrapper">
    <cover class="cover  ">
      
        
  <h1 class="title">Oooooops! Get away from me!</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder>
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class="menu navgation">
  <ul class="h-list">
    
      
        <li>
          <a class="nav home" href="/" id="home">
            <i class="fas fa-rss fa-fw"></i>&nbsp;博文
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/projects/" id="projects">
            <i class="fas fa-code-branch fa-fw"></i>&nbsp;项目
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/friends/" rel="nofollow" id="friends">
            <i class="fas fa-link fa-fw"></i>&nbsp;友链
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about/" rel="nofollow" id="about">
            <i class="fas fa-info-circle fa-fw"></i>&nbsp;关于
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>
<script src="https://unpkg.com/scrollreveal/dist/scrollreveal.min.js"></script>
	<script src="/js/three.r92.min.js"></script>
<script src="/js/vanta.waves.min.js"></script>
<script src="/js/vanta.birds.min.js"></script>
	<script>
		VANTA.BIRDS({
  			el:".cover-wrapper",
  			backgroundColor: 0xffffff,
  			alignment: 100
		})
</script>
	<div class="wrapper">
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href="/">
        
          Chris
        
      </a>
			<div class="menu navgation">
				<ul class="h-list">
          
  					
  						<li>
								<a class="nav flat-box" href="/" id="home">
									<i class="fas fa-grin fa-fw"></i>&nbsp;主页
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/categories/" rel="nofollow" id="categories">
									<i class="fas fa-folder-open fa-fw"></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/tags/" rel="nofollow" id="tags">
									<i class="fas fa-hashtag fa-fw"></i>&nbsp;标签
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/archives/" rel="nofollow" id="archives">
									<i class="fas fa-archive fa-fw"></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search">
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class="switcher h-list">
				
					<li class="s-search"><a class="fas fa-search fa-fw" href="javascript:void(0)"></a></li>
				
				<li class="s-menu"><a class="fas fa-bars fa-fw" href="javascript:void(0)"></a></li>
			</ul>
		</div>

		<div class="nav-sub container container--flex">
			<a class="logo flat-box"></a>
			<ul class="switcher h-list">
				<li class="s-comment"><a class="flat-btn fas fa-comments fa-fw" href="javascript:void(0)"></a></li>
        
          <li class="s-toc"><a class="flat-btn fas fa-list fa-fw" href="javascript:void(0)"></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/" id="home">
								<i class="fas fa-clock fa-fw"></i>&nbsp;近期文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/archives/" rel="nofollow" id="archives">
								<i class="fas fa-archive fa-fw"></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/projects/" id="projects">
								<i class="fas fa-code-branch fa-fw"></i>&nbsp;开源项目
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/friends/" rel="nofollow" id="friends">
								<i class="fas fa-link fa-fw"></i>&nbsp;我的友链
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="https://xaoxuu.com/wiki/material-x/" rel="nofollow" id="https:xaoxuu.comwikimaterial-x">
								<i class="fas fa-book fa-fw"></i>&nbsp;主题文档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/" rel="nofollow" id="about">
								<i class="fas fa-info-circle fa-fw"></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class="l_main">
  
  <section class="post-list">
    
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
    
    
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/12/Langrange/">
      Langrange Multipliers for Constrained Optimization
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-12</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/machine-learning/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>machine learning</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="We-have-gone-through-a-few-optimization-methods-for-unconstrained-optimization-However-in-reality-tremendous-problems-are-factored-into-a-serires-of-constrained-optimization-problems-Therefore-in-this-post-I-will-mainly-introduce-a-widely-employed-method-in-various-machine-learning-algorithms"><a href="#We-have-gone-through-a-few-optimization-methods-for-unconstrained-optimization-However-in-reality-tremendous-problems-are-factored-into-a-serires-of-constrained-optimization-problems-Therefore-in-this-post-I-will-mainly-introduce-a-widely-employed-method-in-various-machine-learning-algorithms" class="headerlink" title="We have gone through a few optimization methods for unconstrained optimization. However, in reality, tremendous problems are factored into a serires of constrained optimization problems. Therefore, in this post, I will mainly introduce a widely employed method in various machine learning algorithms."></a>We have gone through a few optimization methods for unconstrained optimization. However, in reality, tremendous problems are factored into a serires of constrained optimization problems. Therefore, in this post, I will mainly introduce a widely employed method in various machine learning algorithms.</h2><h2 id="Langrange-Multiplier"><a href="#Langrange-Multiplier" class="headerlink" title="Langrange Multiplier"></a>Langrange Multiplier</h2><p>###Intrduction </p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/machine-learning-unconstrained-optimization/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>machine learning, unconstrained optimization</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/11/Proximal-policy-optimization/">
      Proximal_policy_optimization
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-11</p>
  </a>
</div>

          
        
          
            

          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      
      
    </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/11/Trust-region-policy-optimization/">
      Trust_region_policy_optimization
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-11</p>
  </a>
</div>

          
        
          
            

          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      
      
    </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/11/Natural-policy-gradient/">
      Natural_policy_gradient
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-11</p>
  </a>
</div>

          
        
          
            

          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      
      
    </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/11/Policy-gradient/">
      Policy_gradient
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-11</p>
  </a>
</div>

          
        
          
            

          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      
      
    </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/11/Optimization-4/">
      Optimization methods(4)
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-11</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/machine-learning/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>machine learning</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>In this post, I will give a brief introduction on another popular optimization method - <strong><em>trust region optimization method</em></strong>. In the last few posts, I elaborated on <em>line search</em>, <em>steepest gradient descent</em> and <em>newton’s method</em>. However,both the optimized steepest gradient descent and newton’s method are based on the assumption that the objective function can be approximated locally by a quadratic function. However, it fails to approximate the objective function ,the descent may leads to the convergence problem as the following two pics show.</p>
<p><figure class="half"><br>    <img src="/images/Trustregion1.png"><br>    <img src="/images/Trustregion2.png"><br></figure></p>
<h2 id="To-address-this-problem-we-will-introduce-trust-region-method-in-this-post"><a href="#To-address-this-problem-we-will-introduce-trust-region-method-in-this-post" class="headerlink" title="To address this problem, we will introduce trust region method in this post."></a>To address this problem, we will introduce <strong>trust region method</strong> in this post.</h2><h2 id="Trust-region-methods"><a href="#Trust-region-methods" class="headerlink" title="Trust region methods"></a>Trust region methods</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><font color="red"><strong>Introduction</strong></font></h3><blockquote>
<p>Trust-region method (TRM) is one of the most important numerical optimization methods in solving nonlinear programming (NLP) problems. It works in a way that first define a region around the current best solution, in which a certain model (usually a quadratic model) can to some extent approximate the original objective function. TRM then take a step forward according to the model depicts within the region. Unlike the line search methods, TRM usually determines the step size before the improving direction (or at the same time). If a notable decrease (our following discussion will based on minimization problems) is gained after the step forward, then the model is believed to be a good representation of the original objective function. If the improvement is too subtle or even a negative improvement is gained, then the model is not to be believed as a good representation of the original objective function within that region. The convergence can be ensured that the size of the “trust region” (usually defined by the radius in Euclidean norm) in each iteration would depend on the improvement previously made.</p>
</blockquote>
<h3 id="Deduction"><a href="#Deduction" class="headerlink" title="Deduction"></a><font color="red"><strong>Deduction</strong></font></h3><p>Consider the unconstrained problem:<br>$$min\;f(x), x \in \mathbb{R}^n$$<br>We compute the second order <strong>Taylor expansion</strong> approximation of the objective at point $x_k$ ,namely:<br>$$f(x)\approx f(x_k)+\nabla f(x_k)^T(x-x_k)+\frac{1}{2}\nabla^2f(x_k)(x-x_k)^2$$<br>Denote $d=x-x_k$, we have the quadratic function:<br>$$\phi_k(d)=f(x_k)+\nabla f(x_k)^Td +\frac{1}{2}d^T\nabla^2 f(x_k)d$$<br>To accurately approximate $f(x_k+d)$ with $\phi_k(d)$ in the vicinity of $x_k$, we enforce constraints on $d$ with $\left|d\right|d \leq r_k$. $r_k$ is a given constant which in this setting is widely referred to as the <strong>trust region radius</strong>. The constraint on $d$ could be various kinds of norms according to different settings. In this post, we will focus on the method which enforces <em>Ecludian norm</em> on $d$, which means $(d^Td)^{\frac{1}{2}}\leq r_k$.<br>If $d_k$ is the optimum solution to the objective, then, there exits a <strong>lagrange mutiplier</strong> $w \geq 0$ satisfying<br>$$ \nabla^2 f(x_k)d_k +\nabla f(x_k) +\frac{w’}{d_k^Td_k}d_k = 0$$<br>$$ w’(||d_k||-r_k) = 0)$$<br>Denote $$ w=\frac{w’}{(d_k^Td_k)^{\frac{1}{2}}}$$</p>
<p>Then the conditions under which $d_k$ is the optimum are(If u have doubts about these equations,search <strong>KKT</strong> conditions on Google or refer to my posts about <strong>Langarange multiplier</strong>):</p>
<p>$$<br>\begin{align}<br>\nabla^2 f(x_k)d_k +wd_k = -\nabla f(x_k) \\<br>w(d_k- r_k) =0  \\<br>w \geq 0 \\<br>||d_k|| \leq r_k  \\<br>\end{align}<br>$$</p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/machine-learning-unconstrained-optimization/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>machine learning, unconstrained optimization</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/09/Optimization-3/">
      Optimization methods(3)
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-09</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/machine-learning/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>machine learning</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>In this post, I will talk about another optimzation method which is called <strong><em>Conjugate gradient descent</em></strong>. The reason for the induction here is that <strong><em>CG</em></strong> method is employed in reinforcement learning but is not well explained in most blogs and books. I have read quite a lot descriptions from various </p>
<h2 id="Conjugate-Gradient-Descent"><a href="#Conjugate-Gradient-Descent" class="headerlink" title="Conjugate Gradient Descent"></a>Conjugate Gradient Descent</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><blockquote>
<p><strong>Conjugate gradient descent</strong> is a prominent iterative method for solving sparse systems of linear equations. It basically optimizes the function in the order of \(N\) A-orthogonal search directions. Intuitively, it minimizes the function along one direction \(d\) to the point where the subsequent minimization steps will never need to minimize over that direction again.</p>
</blockquote>
<h3 id="Conjugacy"><a href="#Conjugacy" class="headerlink" title="Conjugacy"></a>Conjugacy</h3><ul>
<li><strong>Conjugation definition</strong></li>
</ul>
<blockquote>
<p>Suppose A is a \(n*n\) positive definite matrix. \(d^1\) and \(d^2\) are two directions in \(R^n\) which satisfy the following equation:<br>$$d^1Ad^2=0$$<br>Then these two directions are called conjugate directions of \(A\).</p>
</blockquote>
<ul>
<li><strong>Conjugate optimization</strong></li>
</ul>
<p>Steepest gradient descent as we all know will take the zigzag path in the optimization process. It will take steps in the direction which has been taken before. Wouldnt it be better if every time we take a step, we have arrived at a poistion where we never would need to step in that direction again. The idea is demonstrated as follows.) </p>
<ol>
<li>Pick a set of orthogonal <em>search directions</em> \(d_0,d_1, …., d_{n-1}\)</li>
<li><p>In each direction, we take exactly one step to minimize the function.</p>
<p>$$x_{i+1}=x_i+\alpha_i d_i$$</p>
</li>
</ol>
<p>We denote the <em>error item</em> after \(i-1\) iterations as \(e_i\), as illustrated in the above picture. To find the value of \(\alpha_i\) , use the fact that \(e_{i+1}\) should be orthogonal to \(d_i\), so that we never step in the direction of \(d_i\) again.  Then, we have</p>
<p>  $$<br>  \begin{split}<br>   d_i^Te_{i+1}=0  \\<br>   d_i^T(e_i+\alpha_id_i)=0 \\<br>   \alpha_i =- \frac{d_i^Te_i}{d_i^Td_i}<br>  \end{split}<br>  $$</p>
<ol start="3">
<li><p>After n steps, we achieve the minimum.</p>
<p>However, the problem is not solved since we dont know about \(e_i\) in advance. Now we relax the <em>orthogonal</em> condition to <em>A-orthogonal,conjugate</em> ,which, as stated before, means that $d_i^TAd_j=0$.</p>
</li>
</ol>
<p><img src="/images/Conjugate1.png" alt="Conjugate directions"><br>We set the directional derivative to zero:</p>
<p>$$<br>\begin{split}<br>  \frac{d}{d\alpha}f(x_{i+1}) \\<br>  f’(x_{i+1})^T\frac{d}{d\alpha}x_{i+1} =0 \\<br>  -r_{i+1}^Td_i =0<br>\end{split}<br>$$</p>
<p>$$ d_i^TAe_{i+1} =0$$<br>Then, here is the expression for $\alpha_i$ when the search directions are <em>A_orthogonal(Conjugate)</em>:<br>$$<br>\begin{align}<br>\alpha_i= -\frac{d_i^TAe_i}{d_i^TAd_i} \<br>        = \frac{d_i^Tr_i}{d_i^TAd_i}<br>\end{align}<br>$$<br>Note that if the search vectors are just the residuals, this formula would be identical to the <em>steepest Descent</em>.<br>To prove this procedure indeed compute x in $n$ steps, we can express the error item as a linear combination of search directions since the search directions are a complete span of the entire search space; namely:</p>
<p>$$e_0 = \sum_{j=0}^{n-1}\delta_j d(j)$$</p>
<p>The corresponding values of each $\delta_i$ can be computed using the fact that the search directions are <em>conjugate</em>. We premultiply the above expression by $d_k^TA$:</p>
<p>$$<br>\begin{align}<br>d_k^TAe_0=\sum_j\delta_j d_k^TAd_j \\<br>d_k^TAe_0=\delta_k d_k^TAd_k  \\<br>  \delta_k= \frac{d_k^TAe_0}{d_k^TAd_k} \\<br>  =\frac{d_k^TA(e_0+\sum_{i=0}^{k-1}\alpha_i d_i)}{d_k^TAd_k} \\<br>  =\frac{d_k^TAe_k}{d_k^TAd_k} \\<br>\end{align}<br>$$<br>By the above equation, we find that $\alpha_i =-\delta_i$. As the following equation shows, the process of building up x component by component can also be viewed as cutting down the error term component by component. See the following Figure.</p>
<p>$$<br>\begin{align}<br>e_i = e_0+\sum_{j=0}^{i-1}\alpha_j d_j \\<br>    = \sum_{j=0}^{n-1}\delta_j d_j - \sum_{j=0}^{i-1}\delta_j d_j \\<br>    =\sum_{j=i}^{n-1}\delta_j d_j \\<br>\end{align}<br>$$</p>
<p>After n iterations, every error term is cut away, and $e_n=0$; the proof is complete. </p>
<p><img src="/images/Conjugate2.png" alt></p>
<ul>
<li><strong>Conjugate directions</strong><br>According to the above demonstration, we have a clear idea about the conjugate optimization. The key point is how to find a set of <em>conjugate directions</em> $d_i$. We will briefly go through a commmon method to tackle this headache which is called <strong>conjugate Gram-Schmidt</strong> process.<br>Suppose we have a set of n linearly independent vectors $u_0,u_1,…,u_{n-1}$. To construct $d_i$, take $u_i$ and substract out any component that are not conjugate to the previous $d$ vectors. See the following fig.<br><img src="/images/Conjugate3.png" alt><br>In other words, set $d_0=u_0$, and for $i&gt;0$, set</li>
</ul>
<p>$$d_i=u_i + \sum_{k=0}^{i-1}\beta_{ik} d_k$$</p>
<p>where the $\beta_{ik}$ are defined for $i&gt;k$. To figure out their values, we employ the same trick for finding $\delta_j$, namely: </p>
<p>$$<br>\begin{align}<br> d_i^TAd_j = u_i^TAd_j +\sum_{k=0}^{i-1}\beta_{ik} d_k^TAd_j \\<br> 0 = u_i^TAd_j +\beta_ {ij}d_j^TAd_j \\<br> \beta_{ij} = -\frac{u_i^TAd_j}{d_j^TAd_j}<br>\end{align}<br>$$<br>The weakness of this method lies in the fact that all old search directions need to be kept in memory to contruct a new direction. </p>
<h3 id="Conjugate-gradient-method"><a href="#Conjugate-gradient-method" class="headerlink" title="Conjugate gradient method"></a><font color="#dd0000">Conjugate gradient method</font><br></h3><p>According to the above demonstration, we need to find a set of independent vectors. In this section, I will demonstrate the <strong>Conjugate gradient method</strong> way to deal with all of these issues. However, since the detailed proof of the intrinsics of the algorithm is too length, I will skip the proof and get hands on the algorithm directly. </p>
<ul>
<li><p>Get the first direction $d_0 = r_0 =b -Ax_0$</p>
</li>
<li><p>Compute the step size in that direction which is $$\alpha_i = \frac { r_i^Tr_i }{ d_i^TAd_i }$$</p>
</li>
<li><p>Get the position of next point $x_{i+1} = x_i +\alpha_i d_i$</p>
</li>
<li><p>Compute the gradient (residual) at $x_{i+1}$ which equals $r_{i+1} = r_i-\alpha_i Ad_i$</p>
</li>
<li><p>Compute the coefficient of $d_i$, namely $$\beta_{i+1} = \frac{r_{i+1}^Tr_{i+1}}{r_i^Tr_i}$$</p>
</li>
<li><p>Get the next search direction $d_{i+1} = r_{i+1} + \beta_{i+1} d_i$</p>
</li>
</ul>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/machine-learning-unconstrained-optimization/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>machine learning, unconstrained optimization</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/07/Diary/">
      Diary
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-07</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/life/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>life</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      The article has been encrypted, please enter your password to view.<br>
      
        <div class="readmore">
          <a href="/2019/05/07/Diary/" class="flat-box">
            <i class="fas fa-book-open fa-fw" aria-hidden="true"></i>
            Read More
          </a>
        </div>
      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/diary-phd-life/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>diary, phd life</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/07/optimization-2/">
      Optimization methods(2)
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-07</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/machine-learning/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>machine learning</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>This is the second post to the general optimization methods. In this post, I will briefly talk about <strong><em>Newton’s method</em></strong> .</p>
<h2 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h2><h3 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h3><blockquote>
<p>In numerical analysis, Newton’s method, also known as the <strong>Newton–Raphson method</strong>, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.</p>
</blockquote>
<h3 id="Deduction"><a href="#Deduction" class="headerlink" title="Deduction"></a>Deduction</h3><p>Suppose we have a function \(f(x)\) which is real and differentiable. We want to find the solutiont to \(f(x)=0\). Lets suppose we have an initial guess for the solution, say \(x_0\). The initial guess is not that satisfactory but we can use it to iteratively get a more accurate guess. Below is the deduction process.</p>
<ol>
<li>We first use <strong>Taylor Expansion</strong> to get the fist order expansion of \(f(x)\)</li>
</ol>
<p>$$y=f(x_0)+f’(x_0)(x-x_0)$$</p>
<ol start="2">
<li><p>Set \(y=0\) , which equals to\(f(x_0)+f’(x_0)(x-x_0)=0\)and we can get the following iterative optimization equation<br>$$x=x_0 - \frac{f(x_0)}{f’(x_0)}$$</p>
</li>
<li><p>By employ the above equation, we can approximate the root of the function \(f(x)=0\) after sufficient iterations.  The <strong>Newton’s Method</strong> is graphically depicted in the picture below.<br><img src="/images/newton.png" alt="newton"> </p>
</li>
</ol>
<h3 id="Newton’s-method-for-optimization"><a href="#Newton’s-method-for-optimization" class="headerlink" title="Newton’s method for optimization"></a>Newton’s method for optimization</h3><p>If we want to minimize a function, say \(f(x)\) instead of searching for the roots of the function. Intuitively, we can employ <strong>Newton’s Method</strong> on the gradient of the function \(f(x)\) since the minimum of the function \(f(x)\) is achieved when the gradient of the function equals to \(0\).</p>
<ul>
<li>We first get the second order <strong>Taylor Expansion</strong> of the function \(f(x)\) at the initial point \(x_0\). Suppose the function is multi-variant.</li>
</ul>
<p>$$f(x)=f(x_0)+\nabla f(x_0)(x-x_0)+ \frac{1}{2}(x-x_0)^T \nabla^2 f(x_0) (x-x_0)$$</p>
<p>\(\nabla^2 f(x_0)\) is the <strong>Hessian Matrix</strong> or <strong>Fisher information matrix)</strong> of \(f(x)\) at \(x_0\). </p>
<ul>
<li>then  we need to apply <strong>Newton’s Method</strong> to find the root of \(\nabla f(x) =0\), which equals to the following :<br>$$\nabla f(x_0)+\nabla^2 f(x_0)(x-x_0)=0$$<br>Then , we can derive the iterative update equation as <strong> \(x_1=x_0 - \frac{\nabla f(x_0)}{\nabla^2 f(x_0)}\) </strong></li>
</ul>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a><strong>Discussion</strong></h3><h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h4><p>The convergence speed is way faster than <strong>Steepest Gradient Descent</strong>.</p>
<h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><p>Need to compute the Hessian, which is rather compuation-intensive. Meanwhile, it relys on the second order Taylor expansion to approximate the original function. Thus, the scenario may happen where the quadratic approximation function.  </p>

      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/machine-learning-unconstrained-optimization/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>machine learning, unconstrained optimization</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
        
          <div class="post-wrapper">
            <article class="post reveal ">
  


  <section class="meta">
    
    
    <div class="meta" id="header-meta">
      
        
  <h2 class="title">
    <a href="/2019/05/07/gradient/">
      Optimization methods(1)
    </a>
  </h2>


      
      <div class="new-meta-box">
        
          
        
          
            
  <div class="new-meta-item author">
    <a href="http://yoursite.com" rel="nofollow">
      
        <i class="fas fa-user" aria-hidden="true"></i>
      
      <p>Chris</p>
    </a>
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class="notlink">
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-05-07</p>
  </a>
</div>

          
        
          
            
  
  <div class="new-meta-item category">
    <a href="/categories/machine-learning/" rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>machine learning</p>
    </a>
  </div>


          
        
          
            

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


  <section class="article typo">
    <div class="article-entry" itemprop="articleBody">
      <p>This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients.</p>
<h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><blockquote>
<p>Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard.</p>
</blockquote>
<h2 id="Optimzation-methods"><a href="#Optimzation-methods" class="headerlink" title="Optimzation methods"></a>Optimzation methods</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a><strong>Gradient Descent</strong></h3><blockquote>
<p>Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. </p>
</blockquote>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Consider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible.</p>
<div style="text-align: center"><br><img src="/images/mountain.png"><br></div>

<p>Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.</p>
<h3 id="An-example"><a href="#An-example" class="headerlink" title="An example"></a><em>An example</em></h3><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>Given the cost function</p>
<p>$$f(m,b)=\frac{1}{N}\sum_{i=1}^{n} (x)$$</p>
<p>The gradient can be calculated as:</p>
<div style="text-align: center"><br><img src="/images/gradient.png"><br></div>

<p>To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.</p>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a><strong>Stochastic Gradient Descent</strong></h3><p>Conputing the gradient over all the data points is way too expensive. Stochastic  gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update.</p>
<h3 id="Newton’s-Method"><a href="#Newton’s-Method" class="headerlink" title="Newton’s Method"></a>Newton’s Method</h3><h3 id="Natural-Gradient"><a href="#Natural-Gradient" class="headerlink" title="Natural Gradient"></a>Natural Gradient</h3>
      
    </div>
    
      <div class="full-width auto-padding tags">
        
          <a href="/tags/machine-learning-optimization/" rel="nofollow"><i class="fas fa-hashtag fa-fw"></i>machine learning, optimization</a>
        
      </div>
    
  </section>
</article>

          </div>
        
      
    
  </section>
  
    
    <!-- 根据主题中的设置决定是否在archive中针对摘要部分的MathJax公式加载mathjax.js文件 -->
    
    
      <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    

  


</div>
<aside class="l_side">
  
    
    
      
        
          
          
            <section class="widget author">
  <div class="content pure">
    
      <div class="avatar">
        <img class="avatar" src="/images/chen_avatar.JPG">
      </div>
    
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml" class="social fas fa-rss flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:me@xaoxuu.com" class="social fas fa-envelope flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/chrisbyd" class="social fab fa-github flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://music.163.com/#/user/home?id=63035382" class="social fas fa-headphones-alt flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

          
        
      
        
          
          
            

          
        
      
        
          
          
            <section class="widget grid">
  
<header class="pure">
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class="content pure">
    <ul class="grid navgation">
      
        <li><a class="flat-box" title="/" href="/" id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" title="/archives/" href="/archives/" rel="nofollow" id="archives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" title="/projects/" href="/projects/" id="projects">
          
            <i class="fas fa-code-branch fa-fw" aria-hidden="true"></i>
          
          开源项目
        </a></li>
      
        <li><a class="flat-box" title="/friends/" href="/friends/" rel="nofollow" id="friends">
          
            <i class="fas fa-link fa-fw" aria-hidden="true"></i>
          
          我的友链
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/material-x/" href="https://xaoxuu.com/wiki/material-x/" rel="nofollow" id="https:xaoxuu.comwikimaterial-x">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          主题文档
        </a></li>
      
        <li><a class="flat-box" title="/about/" href="/about/" rel="nofollow" id="about">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

          
        
      
        
          
          
            
  <section class="widget category">
    
<header class="pure">
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;Categories</div>
  
    <a class="rightBtn" rel="nofollow" href="/blog/categories/" title="blog/categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class="content pure">
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/life/" href="/categories/life/"><div class="name">life</div><div class="badge">(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/machine-learning/" href="/categories/machine-learning/"><div class="name">machine learning</div><div class="badge">(5)</div></a></li>
        
      </ul>
    </div>
  </section>


          
        
      
        
          
          
            
  <section class="widget tagcloud">
    
<header class="pure">
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;Hot Tags</div>
  
    <a class="rightBtn" rel="nofollow" href="/blog/tags/" title="blog/tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class="content pure">
      <a href="/tags/diary-phd-life/" style="font-size: 14px; color: #999">diary, phd life</a> <a href="/tags/machine-learning-optimization/" style="font-size: 14px; color: #999">machine learning, optimization</a> <a href="/tags/machine-learning-unconstrained-optimization/" style="font-size: 24px; color: #555">machine learning, unconstrained optimization</a>
    </div>
  </section>


          
        
      
        
          
          
            


  <section class="widget music">
    
<header class="pure">
  <div><i class="fas fa-compact-disc fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;最近在听</div>
  
    <a class="rightBtn" rel="external nofollow noopener noreferrer" target="_blank" href="https://music.163.com/#/user/home?id=63035382" title="https://music.163.com/#/user/home?id=63035382">
    <i class="far fa-heart fa-fw"></i></a>
  
</header>

    <div class="content pure">
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer" data-theme="#1BCDFC" data-mode="circulation" data-server="netease" data-type="playlist" data-id="2615636388" data-volume="0.7">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    </div>
  </section>


          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml" class="social fas fa-rss flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:me@xaoxuu.com" class="social fas fa-envelope flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/chrisbyd" class="social fab fa-github flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://music.163.com/#/user/home?id=63035382" class="social fas fa-headphones-alt flat-btn" target="_blank" rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>Blog content follows the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
  <div>
    Use
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    as theme
    
      , 
      total visits
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      times
    
    . 
  </div>
</footer>
<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>














  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.4.19/js/app.js"></script>


  <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.4.19/js/search.js"></script>




<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "Copied";
  let COPY_FAILURE = "Copy failed";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>Copy</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
