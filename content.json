{"meta":{"title":"Chris","subtitle":"Life in SJTU as a PhD in ML","description":"This is the place for chris to write down his daily feelings and some technical articles","author":"Chris","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-05-07T15:44:39.000Z","updated":"2019-05-07T15:44:39.871Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-05-07T15:30:47.000Z","updated":"2019-05-07T15:33:34.706Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-07T15:34:25.000Z","updated":"2019-05-07T15:34:49.882Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"optimization_2","slug":"optimization-2","date":"2019-05-07T05:36:21.000Z","updated":"2019-05-07T13:55:51.577Z","comments":true,"path":"2019/05/07/optimization-2/","link":"","permalink":"http://yoursite.com/2019/05/07/optimization-2/","excerpt":"","text":"This is the second post to the general optimization methods. In this post, I will briefly talk about Newton’s method . Newton’s methodDescriptionIn numerical analysis, Newton’s method, also known as the Newton–Raphson method, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, optimization","slug":"machine-learning-optimization","permalink":"http://yoursite.com/tags/machine-learning-optimization/"}]},{"title":"Optimization methods(1)","slug":"gradient","date":"2019-05-07T02:40:08.000Z","updated":"2019-05-08T04:14:47.039Z","comments":true,"path":"2019/05/07/gradient/","link":"","permalink":"http://yoursite.com/2019/05/07/gradient/","excerpt":"","text":"This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients. Convex Optimization Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard. Optimzation methodsGradient Descent Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. IntroductionConsider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible. Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum. An example Given the cost function $$f(m,b)=\\frac{1}{N}\\sum_{i=1}^{n} (x)$$ The gradient can be calculated as: To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate. Stochastic Gradient DescentConputing the gradient over all the data points is way too expensive. Stochastic gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update. Newton’s MethodNatural Gradient","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, optimization","slug":"machine-learning-optimization","permalink":"http://yoursite.com/tags/machine-learning-optimization/"}]}]}