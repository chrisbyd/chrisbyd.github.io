{"meta":{"title":"Chris","subtitle":"Life in SJTU as a PhD in ML","description":"This is the place for chris to write down his daily feelings and some technical articles","author":"Chris","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-05-07T15:44:39.000Z","updated":"2019-05-07T15:44:39.871Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-05-07T15:30:47.000Z","updated":"2019-05-07T15:33:34.706Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-07T15:34:25.000Z","updated":"2019-05-07T15:34:49.882Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Optimization methods(2)","slug":"optimization-2","date":"2019-05-07T05:36:21.000Z","updated":"2019-05-09T07:42:40.825Z","comments":true,"path":"2019/05/07/optimization-2/","link":"","permalink":"http://yoursite.com/2019/05/07/optimization-2/","excerpt":"","text":"This is the second post to the general optimization methods. In this post, I will briefly talk about Newton’s method . Newton’s methodDescription In numerical analysis, Newton’s method, also known as the Newton–Raphson method, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. DeductionSuppose we have a function \\(f(x)\\) which is real and differentiable. We want to find the solutiont to \\(f(x)=0\\). Lets suppose we have an initial guess for the solution, say \\(x_0\\). The initial guess is not that satisfactory but we can use it to iteratively get a more accurate guess. Below is the deduction process. We first use Taylor Expansion to get the fist order expansion of \\(f(x)\\) $$y=f(x_0)+f’(x_0)(x-x_0)$$ Set \\(y=0\\) , which equals to\\(f(x_0)+f’(x_0)(x-x_0)=0\\)and we can get the following iterative optimization equation$$x=x_0 - \\frac{f(x_0)}{f’(x_0)}$$ By employ the above equation, we can approximate the root of the function \\(f(x)=0\\) after sufficient iterations. The Newton’s Method is graphically depicted in the picture below. Newton’s method for optimizationIf we want to minimize a function, say \\(f(x)\\) instead of searching for the roots of the function. Intuitively, we can employ Newton’s Method on the gradient of the function \\(f(x)\\) since the minimum of the function \\(f(x)\\) is achieved when the gradient of the function equals to \\(0\\). We first get the second order Taylor Expansion of the function \\(f(x)\\) at the initial point \\(x_0\\). Suppose the function is multi-variant. $$f(x)=f(x_0)+\\nabla f(x_0)(x-x_0)+ \\frac{1}{2}(x-x_0)^T \\nabla^2 f(x_0) (x-x_0)$$ \\(\\nabla^2 f(x_0)\\) is the Hessian Matrix or Fisher information matrix) of \\(f(x)\\) at \\(x_0\\). then we need to apply Newton’s Method to find the root of \\(\\nabla f(x) =0\\), which equals to the following :$$\\nabla f(x_0)+\\nabla^2 f(x_0)(x-x_0)=0$$Then , we can derive the iterative update equation as \\(x_1=x_0 - \\frac{\\nabla f(x_0)}{\\nabla^2 f(x_0)}\\) DiscussionAdvantagesThe convergence speed is way faster than Steepest Gradient Descent. DisadvantagesNeed to compute the Hessian, which is rather compuation-intensive.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, unconstrained optimization","slug":"machine-learning-unconstrained-optimization","permalink":"http://yoursite.com/tags/machine-learning-unconstrained-optimization/"}]},{"title":"Optimization methods(1)","slug":"gradient","date":"2019-05-07T02:40:08.000Z","updated":"2019-05-08T04:14:47.039Z","comments":true,"path":"2019/05/07/gradient/","link":"","permalink":"http://yoursite.com/2019/05/07/gradient/","excerpt":"","text":"This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients. Convex Optimization Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard. Optimzation methodsGradient Descent Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. IntroductionConsider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible. Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum. An example Given the cost function $$f(m,b)=\\frac{1}{N}\\sum_{i=1}^{n} (x)$$ The gradient can be calculated as: To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate. Stochastic Gradient DescentConputing the gradient over all the data points is way too expensive. Stochastic gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update. Newton’s MethodNatural Gradient","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, optimization","slug":"machine-learning-optimization","permalink":"http://yoursite.com/tags/machine-learning-optimization/"}]}]}