{"meta":{"title":"Chris","subtitle":"Life in SJTU as a PhD in ML","description":"This is the place for chris to write down his daily feelings and some technical articles","author":"Chris","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-05-07T15:44:39.000Z","updated":"2019-05-07T15:44:39.871Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-05-07T15:30:47.000Z","updated":"2019-05-07T15:33:34.706Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-07T15:34:25.000Z","updated":"2019-05-07T15:34:49.882Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Langrange Multipliers for Constrained Optimization","slug":"Langrange","date":"2019-05-12T04:07:43.000Z","updated":"2019-05-12T05:39:32.820Z","comments":true,"path":"2019/05/12/Langrange/","link":"","permalink":"http://yoursite.com/2019/05/12/Langrange/","excerpt":"","text":"We have gone through a few optimization methods for unconstrained optimization. However, in reality, tremendous problems are factored into a serires of constrained optimization problems. Therefore, in this post, I will mainly introduce a widely employed method in various machine learning algorithms.Langrange Multiplier###Intrduction","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, unconstrained optimization","slug":"machine-learning-unconstrained-optimization","permalink":"http://yoursite.com/tags/machine-learning-unconstrained-optimization/"}]},{"title":"Proximal_policy_optimization","slug":"Proximal-policy-optimization","date":"2019-05-11T12:45:38.000Z","updated":"2019-05-11T12:45:38.381Z","comments":true,"path":"2019/05/11/Proximal-policy-optimization/","link":"","permalink":"http://yoursite.com/2019/05/11/Proximal-policy-optimization/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Trust_region_policy_optimization","slug":"Trust-region-policy-optimization","date":"2019-05-11T12:43:33.000Z","updated":"2019-05-11T12:43:33.036Z","comments":true,"path":"2019/05/11/Trust-region-policy-optimization/","link":"","permalink":"http://yoursite.com/2019/05/11/Trust-region-policy-optimization/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Natural_policy_gradient","slug":"Natural-policy-gradient","date":"2019-05-11T12:43:19.000Z","updated":"2019-05-11T12:43:19.187Z","comments":true,"path":"2019/05/11/Natural-policy-gradient/","link":"","permalink":"http://yoursite.com/2019/05/11/Natural-policy-gradient/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Policy_gradient","slug":"Policy-gradient","date":"2019-05-11T12:43:05.000Z","updated":"2019-05-11T12:43:05.571Z","comments":true,"path":"2019/05/11/Policy-gradient/","link":"","permalink":"http://yoursite.com/2019/05/11/Policy-gradient/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Optimization methods(4)","slug":"Optimization-4","date":"2019-05-11T06:02:32.000Z","updated":"2019-05-12T14:26:49.284Z","comments":true,"path":"2019/05/11/Optimization-4/","link":"","permalink":"http://yoursite.com/2019/05/11/Optimization-4/","excerpt":"","text":"In this post, I will give a brief introduction on another popular optimization method - trust region optimization method. In the last few posts, I elaborated on line search, steepest gradient descent and newton’s method. However,both the optimized steepest gradient descent and newton’s method are based on the assumption that the objective function can be approximated locally by a quadratic function. However, it fails to approximate the objective function ,the descent may leads to the convergence problem as the following two pics show. To address this problem, we will introduce trust region method in this post.Trust region methodsIntroduction Trust-region method (TRM) is one of the most important numerical optimization methods in solving nonlinear programming (NLP) problems. It works in a way that first define a region around the current best solution, in which a certain model (usually a quadratic model) can to some extent approximate the original objective function. TRM then take a step forward according to the model depicts within the region. Unlike the line search methods, TRM usually determines the step size before the improving direction (or at the same time). If a notable decrease (our following discussion will based on minimization problems) is gained after the step forward, then the model is believed to be a good representation of the original objective function. If the improvement is too subtle or even a negative improvement is gained, then the model is not to be believed as a good representation of the original objective function within that region. The convergence can be ensured that the size of the “trust region” (usually defined by the radius in Euclidean norm) in each iteration would depend on the improvement previously made. DeductionConsider the unconstrained problem:$$min\\;f(x), x \\in \\mathbb{R}^n$$We compute the second order Taylor expansion approximation of the objective at point $x_k$ ,namely:$$f(x)\\approx f(x_k)+\\nabla f(x_k)^T(x-x_k)+\\frac{1}{2}\\nabla^2f(x_k)(x-x_k)^2$$Denote $d=x-x_k$, we have the quadratic function:$$\\phi_k(d)=f(x_k)+\\nabla f(x_k)^Td +\\frac{1}{2}d^T\\nabla^2 f(x_k)d$$To accurately approximate $f(x_k+d)$ with $\\phi_k(d)$ in the vicinity of $x_k$, we enforce constraints on $d$ with $\\left|d\\right|d \\leq r_k$. $r_k$ is a given constant which in this setting is widely referred to as the trust region radius. The constraint on $d$ could be various kinds of norms according to different settings. In this post, we will focus on the method which enforces Ecludian norm on $d$, which means $(d^Td)^{\\frac{1}{2}}\\leq r_k$.If $d_k$ is the optimum solution to the objective, then, there exits a lagrange mutiplier $w \\geq 0$ satisfying$$ \\nabla^2 f(x_k)d_k +\\nabla f(x_k) +\\frac{w’}{d_k^Td_k}d_k = 0$$$$ w’(||d_k||-r_k) = 0)$$Denote $$ w=\\frac{w’}{(d_k^Td_k)^{\\frac{1}{2}}}$$ Then the conditions under which $d_k$ is the optimum are(If u have doubts about these equations,search KKT conditions on Google or refer to my posts about Langarange multiplier): $$\\begin{align}\\nabla^2 f(x_k)d_k +wd_k = -\\nabla f(x_k) \\\\w(d_k- r_k) =0 \\\\w \\geq 0 \\\\||d_k|| \\leq r_k \\\\\\end{align}$$","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, unconstrained optimization","slug":"machine-learning-unconstrained-optimization","permalink":"http://yoursite.com/tags/machine-learning-unconstrained-optimization/"}]},{"title":"Optimization methods(3)","slug":"Optimization-3","date":"2019-05-09T07:49:52.000Z","updated":"2019-05-11T06:02:08.307Z","comments":true,"path":"2019/05/09/Optimization-3/","link":"","permalink":"http://yoursite.com/2019/05/09/Optimization-3/","excerpt":"","text":"In this post, I will talk about another optimzation method which is called Conjugate gradient descent. The reason for the induction here is that CG method is employed in reinforcement learning but is not well explained in most blogs and books. I have read quite a lot descriptions from various Conjugate Gradient DescentDefinition Conjugate gradient descent is a prominent iterative method for solving sparse systems of linear equations. It basically optimizes the function in the order of \\(N\\) A-orthogonal search directions. Intuitively, it minimizes the function along one direction \\(d\\) to the point where the subsequent minimization steps will never need to minimize over that direction again. Conjugacy Conjugation definition Suppose A is a \\(n*n\\) positive definite matrix. \\(d^1\\) and \\(d^2\\) are two directions in \\(R^n\\) which satisfy the following equation:$$d^1Ad^2=0$$Then these two directions are called conjugate directions of \\(A\\). Conjugate optimization Steepest gradient descent as we all know will take the zigzag path in the optimization process. It will take steps in the direction which has been taken before. Wouldnt it be better if every time we take a step, we have arrived at a poistion where we never would need to step in that direction again. The idea is demonstrated as follows.) Pick a set of orthogonal search directions \\(d_0,d_1, …., d_{n-1}\\) In each direction, we take exactly one step to minimize the function. $$x_{i+1}=x_i+\\alpha_i d_i$$ We denote the error item after \\(i-1\\) iterations as \\(e_i\\), as illustrated in the above picture. To find the value of \\(\\alpha_i\\) , use the fact that \\(e_{i+1}\\) should be orthogonal to \\(d_i\\), so that we never step in the direction of \\(d_i\\) again. Then, we have $$ \\begin{split} d_i^Te_{i+1}=0 \\\\ d_i^T(e_i+\\alpha_id_i)=0 \\\\ \\alpha_i =- \\frac{d_i^Te_i}{d_i^Td_i} \\end{split} $$ After n steps, we achieve the minimum. However, the problem is not solved since we dont know about \\(e_i\\) in advance. Now we relax the orthogonal condition to A-orthogonal,conjugate ,which, as stated before, means that $d_i^TAd_j=0$. We set the directional derivative to zero: $$\\begin{split} \\frac{d}{d\\alpha}f(x_{i+1}) \\\\ f’(x_{i+1})^T\\frac{d}{d\\alpha}x_{i+1} =0 \\\\ -r_{i+1}^Td_i =0\\end{split}$$ $$ d_i^TAe_{i+1} =0$$Then, here is the expression for $\\alpha_i$ when the search directions are A_orthogonal(Conjugate):$$\\begin{align}\\alpha_i= -\\frac{d_i^TAe_i}{d_i^TAd_i} \\ = \\frac{d_i^Tr_i}{d_i^TAd_i}\\end{align}$$Note that if the search vectors are just the residuals, this formula would be identical to the steepest Descent.To prove this procedure indeed compute x in $n$ steps, we can express the error item as a linear combination of search directions since the search directions are a complete span of the entire search space; namely: $$e_0 = \\sum_{j=0}^{n-1}\\delta_j d(j)$$ The corresponding values of each $\\delta_i$ can be computed using the fact that the search directions are conjugate. We premultiply the above expression by $d_k^TA$: $$\\begin{align}d_k^TAe_0=\\sum_j\\delta_j d_k^TAd_j \\\\d_k^TAe_0=\\delta_k d_k^TAd_k \\\\ \\delta_k= \\frac{d_k^TAe_0}{d_k^TAd_k} \\\\ =\\frac{d_k^TA(e_0+\\sum_{i=0}^{k-1}\\alpha_i d_i)}{d_k^TAd_k} \\\\ =\\frac{d_k^TAe_k}{d_k^TAd_k} \\\\\\end{align}$$By the above equation, we find that $\\alpha_i =-\\delta_i$. As the following equation shows, the process of building up x component by component can also be viewed as cutting down the error term component by component. See the following Figure. $$\\begin{align}e_i = e_0+\\sum_{j=0}^{i-1}\\alpha_j d_j \\\\ = \\sum_{j=0}^{n-1}\\delta_j d_j - \\sum_{j=0}^{i-1}\\delta_j d_j \\\\ =\\sum_{j=i}^{n-1}\\delta_j d_j \\\\\\end{align}$$ After n iterations, every error term is cut away, and $e_n=0$; the proof is complete. Conjugate directionsAccording to the above demonstration, we have a clear idea about the conjugate optimization. The key point is how to find a set of conjugate directions $d_i$. We will briefly go through a commmon method to tackle this headache which is called conjugate Gram-Schmidt process.Suppose we have a set of n linearly independent vectors $u_0,u_1,…,u_{n-1}$. To construct $d_i$, take $u_i$ and substract out any component that are not conjugate to the previous $d$ vectors. See the following fig.In other words, set $d_0=u_0$, and for $i&gt;0$, set $$d_i=u_i + \\sum_{k=0}^{i-1}\\beta_{ik} d_k$$ where the $\\beta_{ik}$ are defined for $i&gt;k$. To figure out their values, we employ the same trick for finding $\\delta_j$, namely: $$\\begin{align} d_i^TAd_j = u_i^TAd_j +\\sum_{k=0}^{i-1}\\beta_{ik} d_k^TAd_j \\\\ 0 = u_i^TAd_j +\\beta_ {ij}d_j^TAd_j \\\\ \\beta_{ij} = -\\frac{u_i^TAd_j}{d_j^TAd_j}\\end{align}$$The weakness of this method lies in the fact that all old search directions need to be kept in memory to contruct a new direction. Conjugate gradient methodAccording to the above demonstration, we need to find a set of independent vectors. In this section, I will demonstrate the Conjugate gradient method way to deal with all of these issues. However, since the detailed proof of the intrinsics of the algorithm is too length, I will skip the proof and get hands on the algorithm directly. Get the first direction $d_0 = r_0 =b -Ax_0$ Compute the step size in that direction which is $$\\alpha_i = \\frac { r_i^Tr_i }{ d_i^TAd_i }$$ Get the position of next point $x_{i+1} = x_i +\\alpha_i d_i$ Compute the gradient (residual) at $x_{i+1}$ which equals $r_{i+1} = r_i-\\alpha_i Ad_i$ Compute the coefficient of $d_i$, namely $$\\beta_{i+1} = \\frac{r_{i+1}^Tr_{i+1}}{r_i^Tr_i}$$ Get the next search direction $d_{i+1} = r_{i+1} + \\beta_{i+1} d_i$","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, unconstrained optimization","slug":"machine-learning-unconstrained-optimization","permalink":"http://yoursite.com/tags/machine-learning-unconstrained-optimization/"}]},{"title":"Optimization methods(2)","slug":"optimization-2","date":"2019-05-07T05:36:21.000Z","updated":"2019-05-09T07:48:45.090Z","comments":true,"path":"2019/05/07/optimization-2/","link":"","permalink":"http://yoursite.com/2019/05/07/optimization-2/","excerpt":"","text":"This is the second post to the general optimization methods. In this post, I will briefly talk about Newton’s method . Newton’s methodDescription In numerical analysis, Newton’s method, also known as the Newton–Raphson method, named after Isaac Newton and Joseph Raphson, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. DeductionSuppose we have a function \\(f(x)\\) which is real and differentiable. We want to find the solutiont to \\(f(x)=0\\). Lets suppose we have an initial guess for the solution, say \\(x_0\\). The initial guess is not that satisfactory but we can use it to iteratively get a more accurate guess. Below is the deduction process. We first use Taylor Expansion to get the fist order expansion of \\(f(x)\\) $$y=f(x_0)+f’(x_0)(x-x_0)$$ Set \\(y=0\\) , which equals to\\(f(x_0)+f’(x_0)(x-x_0)=0\\)and we can get the following iterative optimization equation$$x=x_0 - \\frac{f(x_0)}{f’(x_0)}$$ By employ the above equation, we can approximate the root of the function \\(f(x)=0\\) after sufficient iterations. The Newton’s Method is graphically depicted in the picture below. Newton’s method for optimizationIf we want to minimize a function, say \\(f(x)\\) instead of searching for the roots of the function. Intuitively, we can employ Newton’s Method on the gradient of the function \\(f(x)\\) since the minimum of the function \\(f(x)\\) is achieved when the gradient of the function equals to \\(0\\). We first get the second order Taylor Expansion of the function \\(f(x)\\) at the initial point \\(x_0\\). Suppose the function is multi-variant. $$f(x)=f(x_0)+\\nabla f(x_0)(x-x_0)+ \\frac{1}{2}(x-x_0)^T \\nabla^2 f(x_0) (x-x_0)$$ \\(\\nabla^2 f(x_0)\\) is the Hessian Matrix or Fisher information matrix) of \\(f(x)\\) at \\(x_0\\). then we need to apply Newton’s Method to find the root of \\(\\nabla f(x) =0\\), which equals to the following :$$\\nabla f(x_0)+\\nabla^2 f(x_0)(x-x_0)=0$$Then , we can derive the iterative update equation as \\(x_1=x_0 - \\frac{\\nabla f(x_0)}{\\nabla^2 f(x_0)}\\) DiscussionAdvantagesThe convergence speed is way faster than Steepest Gradient Descent. DisadvantagesNeed to compute the Hessian, which is rather compuation-intensive. Meanwhile, it relys on the second order Taylor expansion to approximate the original function. Thus, the scenario may happen where the quadratic approximation function.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, unconstrained optimization","slug":"machine-learning-unconstrained-optimization","permalink":"http://yoursite.com/tags/machine-learning-unconstrained-optimization/"}]},{"title":"Optimization methods(1)","slug":"gradient","date":"2019-05-07T02:40:08.000Z","updated":"2019-05-08T04:14:47.039Z","comments":true,"path":"2019/05/07/gradient/","link":"","permalink":"http://yoursite.com/2019/05/07/gradient/","excerpt":"","text":"This is an introduction blog to natural gradient. I hope it will serve as a stepping stone for machine learning and reinforcement learning beginners to understand the basic underlying mathematical foundations behind gradients. Convex Optimization Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Whereas many classes of convex optimization problems admit polynomial-time algorithms, mathematical optimization is in general NP-hard. Optimzation methodsGradient Descent Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks. IntroductionConsider the 3-dimensional graph below in the context of a cost function. Our goal is to move from the mountain in the top right corner (high cost) to the dark blue sea in the bottom left (low cost). The arrows represent the direction of steepest descent (negative gradient) from any given point–the direction that decreases the cost function as quickly as possible. Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum. An example Given the cost function $$f(m,b)=\\frac{1}{N}\\sum_{i=1}^{n} (x)$$ The gradient can be calculated as: To solve for the gradient, we iterate through our data points using our new m and b values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate. Stochastic Gradient DescentConputing the gradient over all the data points is way too expensive. Stochastic gradient descent (SGD) randomly choose a specific number of data points from the entire dataset to compute the gradient and then perform the gradient update. Newton’s MethodNatural Gradient","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/categories/machine-learning/"}],"tags":[{"name":"machine learning, optimization","slug":"machine-learning-optimization","permalink":"http://yoursite.com/tags/machine-learning-optimization/"}]}]}